From your output, the cluster is up, but **no CNI plugin is properly running**, which explains:

* `calico-node` is in `Init:CrashLoopBackOff`
* `calico-kube-controllers` is `Pending`
* `coredns` pods are `Pending`
* Node shows `NotReady` with `NetworkPluginNotReady`

This is **classic Kubernetes bootstrap failure due to missing or broken CNI**. Without a working CNI:

* Pods cannot get an IP.
* CoreDNS cannot schedule.
* Node stays `NotReady`.

---

### **Steps to fix this**

Since you want a **fresh setup**, hereâ€™s what to do to clean Calico/CNI and restart properly:

---

#### **1. Remove old CNI and Calico remnants**

```bash
sudo rm -rf /etc/cni/net.d
sudo rm -rf /opt/cni/bin
sudo rm -rf /var/lib/cni/
sudo rm -rf /var/lib/calico
sudo rm -rf /var/run/calico
sudo rm -rf /var/run/nodeagent
```
The errors youâ€™re seeing are expected when trying to remove cgroup files under `/var/run/calico/cgroup` (or any container runtime-managed cgroup paths) manually using `rm`. These files are **managed by the kernel and container runtime**, so normal `rm` operationsâ€”even as rootâ€”cannot delete them. Attempting to forcibly remove them will always produce `Operation not permitted`.

Hereâ€™s what you need to know and do:

---

### **Why this happens**

1. **Cgroup files are kernel-managed**:

   * Files like `memory.current`, `cpu.max`, `pids.max` are not regular filesâ€”they represent kernel cgroup control and statistics.
   * Deleting them manually is not allowed; the kernel prevents it to maintain process isolation and resource accounting.

2. **Managed by container runtimes**:

   * Container runtimes like **containerd**, **CRI-O**, or **Docker** create these cgroups for pods/containers.
   * Manually deleting the directories/files wonâ€™t workâ€”you need to stop/remove the containers or pods first.

---

### **Safe ways to clean these directories**

#### **1. Delete the Kubernetes pods**

If these cgroups are for Kubernetes pods:

```bash
kubectl get pods -A | grep <pod-name-or-namespace>
kubectl delete pod <pod-name> -n <namespace>
```

Deleting the pod properly will automatically clean up the cgroup directories under `/var/run/calico/cgroup`.

---

#### **2. Stop the container manually via containerd**

If you want to remove it without Kubernetes:

```bash
# List containers
sudo ctr -n k8s.io containers list

# Stop a container
sudo ctr -n k8s.io tasks kill <container-id>
sudo ctr -n k8s.io tasks delete <container-id>
```

After stopping the container, the cgroup entries should automatically disappear.

---

#### **3. Reboot the node**

If there are stale or orphaned cgroups (for example, after container runtime crashes):

* Rebooting the node will clear `/var/run/calico/cgroup` since itâ€™s a runtime-managed tmpfs.

---

### âš ï¸ **Important**

* **Do not use `rm -rf`** on `/var/run/calico/cgroup` directly. That can corrupt the container runtime state.
* Always clean up through the **container runtime** or **Kubernetes**.

---

If you want, I can give you a **step-by-step command set to safely clean up all stale pod cgroups on this node** without rebooting.

Do you want me to provide that?

---

#### **2. Reset kubeadm cluster (if needed)**

> Only do this if you want to restart fully from scratch.

```bash
sudo kubeadm reset -f
sudo systemctl restart kubelet
```

---

#### **3. Reapply Calico CNI**

```bash
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.29.2/manifests/tigera-operator.yaml
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.29.2/manifests/custom-resources.yaml
```

> Make sure `kubectl get pods -n kube-system` shows all Calico pods starting and eventually `Running`.

---

#### **4. Wait for node to become Ready**

```bash
kubectl get nodes -w
```

Node should switch from `NotReady` â†’ `Ready`.

---

#### **5. Check CoreDNS**

```bash
kubectl get pods -n kube-system
kubectl describe pod <coredns-pod> -n kube-system
```

Once CNI works, CoreDNS will schedule and become `Running`.

---

ðŸ’¡ **Key point:**
Your previous issue with `Init:Error` on calico-node is due to:

* CNI binaries missing (`/opt/cni/bin` empty)
* Possibly leftover network config files
* Calico containers cannot initialize IPAM/network.

Cleaning `/etc/cni/net.d`, `/opt/cni/bin`, `/var/lib/calico`, then reapplying Calico fixes this in most cases.

---

If you want, I can give you a **full ready-to-run sequence** to completely clean Kubernetes, containerd, and CNI/Calico and **reinitialize a working single-node cluster** from scratch.

Do you want me to do that?

