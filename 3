Nice — the events give us a clear error chain:

* The Calico **kube-controllers** pod is running and assigned a pod IP (so the API/datastore side is fine).
* The **calico-node** daemonset is stuck in `Init:2/3` and kubelet is failing to create pod sandboxes with the calico CNI.
* Key error from the kubelet events:

  ```
  plugin type="calico" failed (add): stat /var/lib/calico/nodename: no such file or directory: check that the calico/node container is running and has mounted /var/lib/calico/
  ```

  and later:

  ```
  error: route ... already exists for an interface other than 'caliXXXXX': route ... already exists
  ```

So Calico’s node component hasn’t created the on-disk state it expects, and there are stale cali* interfaces / routes conflicting with new sandbox setup. Below is a safe, ordered runbook you can execute now to bring the node to Ready.

---

## Safety first (read this)

* These commands are intended for a single-node cluster you’re actively debugging. If this is production, **back up** before making changes and run commands during a maintenance window.
* Don’t remove `/sys/fs/cgroup` or other kernel-managed paths.

---

## Step A — Inspect calico-node init failures (do this first)

Get exact init container status and logs:

```bash
kubectl -n kube-system describe pod calico-node-ft82g
kubectl -n kube-system get pods -n kube-system -o wide | grep calico-node
# View logs for the init containers (list container names with describe if needed)
kubectl -n kube-system logs calico-node-ft82g -c install-cni         # typical init container name
kubectl -n kube-system logs calico-node-ft82g -c node                # main container
# if other init containers exist, run logs for each
```

Read the logs. If they show permissions, mount, or missing-file errors, proceed to the host checks below.

---

## Step B — Ensure host directories and mounts exist and are writable

Calico expects `/var/lib/calico` (and CNI paths) to be available on the host and mounted into the pod via hostPath.

On the host (root):

```bash
# confirm directory exists and ownership/perm
sudo ls -la /var/lib/calico
sudo stat /var/lib/calico || echo "missing"

# create if missing (safe)
sudo mkdir -p /var/lib/calico
sudo chown root:root /var/lib/calico
sudo chmod 755 /var/lib/calico

# check CNI directories
ls -la /etc/cni/net.d || sudo mkdir -p /etc/cni/net.d
ls -la /opt/cni/bin || sudo mkdir -p /opt/cni/bin
sudo chown -R root:root /etc/cni /opt/cni
```

Why: the error `stat /var/lib/calico/nodename: no such file or directory` means the calico `node` container cannot create/read that file in the mounted hostPath. Creating the directory prevents hostPath mount failures.

---

## Step C — Check the calico-node DaemonSet spec for hostPath mount correctness

```bash
kubectl -n kube-system get ds calico-node -o yaml | sed -n '1,240p'
```

Look for a volume like:

```yaml
volumes:
- name: varlibcalico
  hostPath:
    path: /var/lib/calico
    type: DirectoryOrCreate
```

If `type` is missing or different, or `path` is wrong, edit the manifest to point to `/var/lib/calico` with `DirectoryOrCreate` and re-apply.

If you change it:

```bash
kubectl -n kube-system delete pod -l k8s-app=calico-node
```

---

## Step D — Check / fix leftover cali* interfaces & duplicate routes

`route ... already exists for an interface other than 'cali...'` indicates stale veth/cali interfaces from previous failed sandboxes. Safely remove stale cali* interfaces on the host **only if they are not used by running containers**.

1. List cali interfaces and routes:

```bash
ip -o link | awk -F': ' '{print $2}' | grep '^cali' || true
ip route show table main | grep 10.57.16. || true
```

2. If you see cali interfaces, verify which PIDs/containers own them:

```bash
# show which network namespace links exist
for i in $(ip -o link | awk -F': ' '{print $2}' | grep '^cali' || true); do
  echo "---- $i ----"
  ip -d link show $i
done
```

3. If the cali interfaces are stale (no corresponding container/sandbox), delete them:

```bash
# delete a specific stale interface
sudo ip link delete cali7fe1acd30df || true
# or loop-delete all cali interfaces (careful!)
for iface in $(ip -o link | awk -F': ' '{print $2}' | grep '^cali' || true); do
  echo "Deleting $iface"
  sudo ip link delete $iface || true
done
```

If `ip link delete` returns "Device or resource busy", the interface is still in use — stop and inspect the container using it before deletion.

4. Remove conflicting route if needed:

```bash
# show conflicting route(s)
ip route show | grep "10.57.16"
# delete a specific stale route (example)
sudo ip route del 10.57.16.2/32 dev cali7dfce3d62a5 || true
```

---

## Step E — Ensure CNI plugin binaries exist

Calico's install-cni init container typically populates `/opt/cni/bin`. If that failed, create the directory and allow Calico to install plugins.

```bash
ls -la /opt/cni/bin
# if empty, make sure directory is writeable:
sudo mkdir -p /opt/cni/bin
sudo chown root:root /opt/cni/bin
sudo chmod 755 /opt/cni/bin
```

If you are air-gapped, you may need to pre-populate CNI plug-ins; otherwise Calico init container should do it.

---

## Step F — Delete/recreate calico-node pods so they re-run init cleanly

After ensuring directories and cleaning stale network interfaces, force the calico-node daemonset pods to be recreated:

```bash
kubectl -n kube-system delete pod -l k8s-app=calico-node
kubectl -n kube-system get pods -w
```

Watch the init containers. If they still fail, immediately check the pod logs (init & main) for the failing init:

```bash
kubectl -n kube-system logs <calico-node-pod-name> -c install-cni
kubectl -n kube-system logs <calico-node-pod-name> -c node
```

---

## Step G — Verify CNI config and kubelet behavior

Once calico-node pods are Running/Ready:

1. Inspect CNI config files on host:

```bash
sudo ls -la /etc/cni/net.d
sudo cat /etc/cni/net.d/*calico*.conf
```

2. Ensure `kubectl get nodes` shows node Ready:

```bash
kubectl get nodes
kubectl -n kube-system get pods
```

3. If pods still fail to create sandboxes, check kubelet logs for plugin errors:

```bash
sudo journalctl -u kubelet -n 200 --no-pager
```

---

## Quick checklist (summary)

* [ ] `/var/lib/calico` exists and is mountable by DaemonSet (hostPath DirectoryOrCreate)
* [ ] `/opt/cni/bin` exists (CNI plugins present or install-cni can write there)
* [ ] No stale `cali*` interfaces or duplicate routes (delete stale ones)
* [ ] Recreate calico-node pods after host fixes
* [ ] Confirm calico-node main container writes `/var/lib/calico/nodename`
* [ ] Coredns pods should then go from ContainerCreating → Running

---

## If it’s still not fixed — paste these outputs and I’ll parse them immediately

Do these commands and paste results (copy-paste — no need to confirm first):

```bash
kubectl -n kube-system get pods -o wide
kubectl -n kube-system describe pod <calico-node-pod-name> -n kube-system
kubectl -n kube-system logs <calico-node-pod-name> -c install-cni
kubectl -n kube-system logs <calico-node-pod-name> -c node
sudo ls -la /var/lib/calico /etc/cni/net.d /opt/cni/bin
ip -o link | awk -F': ' '{print $2}' | grep '^cali' || true
ip route show | sed -n '1,200p' | grep 10.57.16 -n || true
sudo journalctl -u kubelet -n 200 --no-pager
```

If you paste the outputs I’ll point to the exact failing line and the minimal command to finish the fix.
