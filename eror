You are trying to re-run `kubeadm init`, but **your old Kubernetes cluster is still running**, including the **old kube-apiserver on port 6443**, and your **old containerd socket is still present or partially broken**.

Letâ€™s fix this cleanly and completely.

---

# âœ… **Step 1: Fully Reset Old Kubernetes Cluster**

Before reinstalling Kubernetes, you **must wipe the previous cluster**.

Run:

```bash
sudo kubeadm reset --force
```

Then clean iptables + CNI:

```bash
sudo systemctl stop kubelet
sudo systemctl stop containerd

sudo iptables -F && sudo iptables -t nat -F && sudo iptables -t mangle -F && sudo iptables -X
sudo ipvsadm --clear

sudo rm -rf /etc/cni/net.d
sudo rm -rf /var/lib/cni/
```

---

# âœ… **Step 2: Stop & Disable Kubelet**

The old kube-apiserver is running because kubelet is still active.

```bash
sudo systemctl stop kubelet
sudo systemctl disable kubelet
```

Verify kube-apiserver dies:

```bash
ps -ef | grep kube-apiserver
```

If still running, kill it:

```bash
sudo kill -9 $(pgrep kube-apiserver)
```

---

# âœ… **Step 3: Completely Remove Containerd (including leftover mounts)**

### 1. **Stop containerd**

```bash
sudo systemctl stop containerd
sudo systemctl disable containerd
```

### 2. **Unmount all containerd mounts**

Containerd creates overlay and tmpfs mounts that block deletion.

Unmount all containerd-related mounts:

```bash
mount | grep containerd
```

Then unmount each:

```bash
sudo umount -l /run/containerd/*
sudo umount -l /var/lib/containerd/*
```

Or run mass unmount safely:

```bash
sudo for i in $(mount | grep containerd | awk '{print $3}'); do umount -l $i; done
```

### 3. **Remove containerd binaries**

```bash
sudo rm -rf /usr/bin/containerd /usr/bin/containerd-shim* /usr/bin/ctr
```

### 4. **Remove containerd directories**

```bash
sudo rm -rf /etc/containerd
sudo rm -rf /var/lib/containerd
sudo rm -rf /run/containerd
```

---

# âœ… **Step 4: Remove Old Kubernetes Data**

These leftover directories make kubeadm think an old cluster exists.

```bash
sudo rm -rf /etc/kubernetes
sudo rm -rf /var/lib/kubelet
sudo rm -rf /var/lib/etcd
sudo rm -rf /etc/kubernetes/manifests
```

---

# ğŸ” **Step 5: Verify Port 6443 is Free**

Run:

```bash
sudo lsof -i :6443
```

If anything still shows, kill it:

```bash
sudo kill -9 <PID>
```

---

# â­ **Now Reinstall Containerd Fresh**

If you want containerd back:

```bash
sudo yum remove containerd.io -y
sudo yum install containerd.io -y
```

Generate config:

```bash
sudo containerd config default | sudo tee /etc/containerd/config.toml
sudo systemctl restart containerd
```

---

# â­ **Now Retry kubeadm init**

Once port 6443 is free and containerd is clean:

```bash
sudo kubeadm init --apiserver-advertise-address=10.50.12.120 --pod-network-cidr=10.57.16.0/23
```

---

# ğŸ“Œ If you want, I can give you a **single combined cleanup script** that removes EVERYTHING in one go.
